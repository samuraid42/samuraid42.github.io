<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Robotics Software Engineer | Sanjar Normuradov </title> <meta name="author" content="Sanjar Normuradov "/> <meta name="description" content="<b>Udacity Nanodegree</b>" /> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/icons/ai.png?4148a31b5cb8ab5ecfb930d16ff50ba3"/> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sanjarnormuradov.github.io/projects/udacity_robotics_swe/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sanjar Normuradov&nbsp;</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Robotics Software Engineer</h1> <p class="post-description"><b>Udacity Nanodegree</b></p> </header> <article> <p>Notes are based on <a href="https://www.udacity.com/course/robotics-software-engineer--nd209">Robotics Software Engineer Udacity Nanodegree</a></p> <h2 id="curriculum">Curriculum:</h2> <h3 id="1-introduction-to-robotics">1. Introduction to Robotics</h3> <p>Essential elements of Robotics:</p> <ol> <li><strong>Perception</strong> through sensors (RGB-D camera, LIDAR, RADAR, encoder, GPS, IMU, microphone, thermometer, barometer, etc).</li> <li><strong>Decision Making</strong> by anaylzing sensor measurements (color, distance, position, relative coordinates, acceleration, sound, temperature, pressure, etc).</li> <li><strong>Action</strong> using actuators (linear / rotary, electric / pneumatic / hydraulic / magnetic / thermal), and other commands (communicate, measure, etc).</li> </ol> <h3 id="2-gazebo-world">2. Gazebo World</h3> <p>Gazebo features:</p> <ol> <li><strong>Dynamics Simulation:</strong> Model a robot’s dynamics with a high-performance physics engine.</li> <li><strong>Advanced 3D Graphics:</strong> Render your environment with high-fidelity graphics, including lighting, shadows, and textures.</li> <li><strong>Sensors:</strong> Add sensors to your robot, generate data, and simulate noise.</li> <li><strong>Plugins:</strong> Write a plugin to interact with your world, robot, or sensor.</li> <li><strong>Model Database:</strong> Download a robot or environment from Gazebo library or build your own through their engine.</li> <li><strong>Socket-Based Communication:</strong> Interact with Gazebo running on a remote server through <a href="https://en.wikipedia.org/wiki/Network_socket">socket-based</a> communication.</li> <li><strong>Cloud Simulation:</strong> Run Gazebo on a server and interact with it through a browser.</li> <li><strong>Command Line Tools:</strong> Control your simulated environment through the command line tools. <br/></li> </ol> <p>Components involved in running an instance of a Gazebo simulation:</p> <ol> <li><strong>Gazebo Server:</strong> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> $ gzserver
</code></pre></div> </div> <p>It is responsible for parsing the description files related to the scene we are trying to simulate, as well as the objects within. It then simulates the complete scene using a physics and sensor engine.</p> </li> <li><strong>Gazebo Client</strong> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> $ gzclient
</code></pre></div> </div> <p>It provides the very essential Graphical Client that connects to the <strong>gzserver</strong> and renders the simulation scene along with useful interactive tools. While you can technically run <strong>gzclient</strong> by itself, but it does nothing at all (except consume your compute resources) as it does not have a <strong>gzserver</strong> to connect to and receive instructions from.<br/> It is a common practice to run <strong>gzserver</strong> first, followed by <strong>gzclient</strong>, allowing some time to initialize the simulation scene, objects within, and associated parameters before rendering it. But it could be combined with</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> $ gazebo
</code></pre></div> </div> </li> <li><strong>World Files</strong> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> $ gazebo &lt;yourworld&gt;.world
</code></pre></div> </div> <p>A <strong>world</strong> file in Gazebo contains all the elements in the simulated environment. These elements are your robot model, its environment, lighting, sensors, and other objects. It is formatted using the <strong>Simulation Description Format</strong> (<a href="http://sdformat.org/spec?ver=1.6&amp;elem=world">SDF</a>) such as</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> &lt;?xml version="1.0" ?&gt;
 &lt;sdf version="1.5"&gt;
     &lt;world name="default"&gt;
         &lt;physics type="ode"&gt;
         ...
         &lt;/physics&gt;
            
         &lt;scene&gt;
         ...
         &lt;/scene&gt;

         &lt;model name="box"&gt;
         ...
         &lt;/model&gt;

         &lt;model name="sphere"&gt;
         ...
         &lt;/model&gt;

         &lt;light name="spotlight"&gt;
         ...
         &lt;/light&gt;

     &lt;/world&gt;
 &lt;/sdf&gt;
</code></pre></div> </div> </li> <li><strong>Model Files</strong><br/> For simplification, you must create a separate <strong>SDF</strong> file of your robot with exactly the same format as your <strong>world</strong> file. This <strong>model</strong> file should only represent a single model (ex: a robot) and can be imported by your <strong>world</strong> file. The reason why you need to keep your model in a separate file is to use it in other projects. To include a <strong>model</strong> file of a robot or any other model inside your <strong>world</strong> file, you can add the following code to the <strong>world’s SDF</strong> file: <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> &lt;include&gt;
     &lt;uri&gt;model://model_file_name&lt;/uri&gt;
 &lt;/include&gt;
</code></pre></div> </div> </li> <li><strong>Environment Variables</strong><br/> There are many environment variables that Gazebo uses, primarily to locate files (world, model, …) and set up communications between gzserver and gzclient such as <code class="language-plaintext highlighter-rouge">GAZEBO_MODEL_PATH:</code> - a list of directories where Gazebo looks to populate a model file.</li> <li><strong>Plugins</strong><br/> To interact with a world, model, or sensor in Gazebo, you can write plugins. These plugins can be either loaded from the command line or added to your <strong>SDF</strong> world file. Include the path to your custom plugins with <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> $ export GAZEBO_PLUGIN_PATH=${GAZEBO_PLUGIN_PATH}:/path/to/compiled/files/of/your/custom/plugins
</code></pre></div> </div> <h4 id="project1-build-my-world-">Project1: Build My World <a href="https://github.com/SanjarNormuradov/RoboticsSoftwareEngineer_Project1"><i class="fa-brands fa-github"></i></a></h4> </li> </ol> <div class="container"> <div class="row"> <div class="align-self-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/project1_gazebo_world-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/project1_gazebo_world-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/project1_gazebo_world-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/project1_gazebo_world.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> </div> <h3 id="3-ros-essentials">3. ROS Essentials</h3> <p>ROS is an open-source software framework for robotics development. <br/> It is not an operating system in the typical sense. But like an OS, it provides a means of communicating with hardware. <br/> It also provides a way for different processes to communicate with one another via message passing. <br/> Lastly, ROS features a slick build and package management system called <strong>catkin</strong>, allowing you to develop and deploy software with ease. <br/> ROS also has tools for visualization, simulation, and analysis, as well as extensive community support and interfaces to numerous powerful software libraries.<br/></p> <p><strong>ROS features</strong>:</p> <ul> <li>Robot’s physical components (sensors, actuators, etc) can be abstracted with ROS <strong>nodes</strong>, which contains specific set of operations.</li> <li><strong>ROS Master</strong> maintains the registry of all the active nodes on a system, allowing nodes to locate one another and communicate.</li> <li>ROS Master hosts the <strong>parameter server</strong> where configuration values and parameters are shared among all nodes.</li> <li>ROS nodes can communicate with each other by passing ROS <strong>messages</strong> over ROS <strong>topics</strong>, so that there exist publisher and subscriber nodes.</li> <li>ROS node can publish and be subscribed to <strong>any number of topics</strong>, constantly monitoring the topics to send/receive messages.</li> <li>ROS nodes can interact with ROS <strong>services</strong> on a one-to-one basis, using <strong>request/response</strong> messages.</li> <li>ROS provides a <strong>RQT graph</strong> tool to show compute graph of nodes, i.e. nodes and their means of communication (services, topics) <br/></li> </ul> <p><strong>ROS Master</strong> process is responsible for:</p> <ul> <li>providing naming and registration services to other running nodes.</li> <li>tracking all publishers and subscribers.</li> <li>aggregating log messages generated by the nodes.</li> <li>facilitating connections between nodes.</li> </ul> <p><strong>ROS basic commands</strong>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">$ roscore</code> - start ROS Master process.</li> <li><code class="language-plaintext highlighter-rouge">$ rosrun &lt;package_name&gt; &lt;executable_node_name&gt;</code></li> <li><code class="language-plaintext highlighter-rouge">$ rosnode list</code> - list of active ROS nodes.</li> <li><code class="language-plaintext highlighter-rouge">$ rostopic list</code> - list of active ROS topics.</li> <li><code class="language-plaintext highlighter-rouge">$ rostopic info &lt;/rostopic/name&gt;</code> - print info (message type, publishers/subscribers) about a specific ROS topic.</li> <li><code class="language-plaintext highlighter-rouge">$ rosmsg info &lt;rosmsg/type&gt;</code> - print detailed info about the content of a specific ROS message.</li> <li><code class="language-plaintext highlighter-rouge">$ rostopic echo &lt;/rostopic/name&gt;</code> - print specific ROS topic’s published messages in real time.</li> <li><code class="language-plaintext highlighter-rouge">$ rosparam set &lt;/parameter/name&gt; &lt;value&gt;</code> - set the ROS parameter</li> </ul> <p><code class="language-plaintext highlighter-rouge">$ roslaunch &lt;package_name&gt; &lt;filename&gt;.launch</code> allows to:</p> <ul> <li>launch the ROS Master and multiple nodes</li> <li>set default parameters on the parameter server</li> <li>automatically re-spawn processes that have died</li> </ul> <p><code class="language-plaintext highlighter-rouge">$ rosdep</code> tool will check for a package’s missing dependencies, download them, and install them.<br/> <code class="language-plaintext highlighter-rouge">$ rosdep check &lt;package_name&gt;</code> - check for missing dependencies in a ROS package.<br/> <code class="language-plaintext highlighter-rouge">$ rosdep install -i &lt;package name&gt;</code> - install packages.<br/></p> <p><strong>ROS package</strong> directory structure:</p> <ul> <li>scripts (python executables)</li> <li>src (C++ source files)</li> <li>msg (for custom message definitions)</li> <li>srv (for service message definitions)</li> <li>include (headers/libraries that are needed as dependencies)</li> <li>config (configuration files)</li> <li>urdf (Universal Robot Description Files)</li> <li>meshes (CAD files in .dae (Collada) or .stl (STereoLithography) format)</li> <li>worlds (XML like files that are used for Gazebo simulation environments)</li> </ul> <p><strong>ROS Publishers &amp; Subscribers:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ros::Publisher pub1 = n.advertise&lt;message_type&gt;("/topic_name", queue_size);
pub1.publish(msg);

ros::Subscriber sub1 = n.subscribe("/topic_name", queue_size, callback_function);
void callback_function(package_name::ServiceName::Request&amp; req, package_name::ServiceName::Response&amp; res) {}
</code></pre></div></div> <p><strong>ROS Services:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ros::ServiceServer service = n.advertiseService(`service_name`, handler);
ros::ServiceClient client = n.serviceClient&lt;package_name::service_file_name&gt;("service_name");
client.call(srv);  // request a service 
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">$ rosservice call &lt;service_name&gt; “request”</code> - service call <code class="language-plaintext highlighter-rouge">$ rossrv show &lt;service_name&gt;</code> - print detailed info (request/response message types) about the service</p> <p><a href="https://github.com/ros/cheatsheet/releases/download/0.0.1/ROScheatsheet_catkin.pdf">ROS Cheatsheet (Download PDF)</a></p> <p><strong>ROS PublishAndSubscribe Class Template:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#include &lt;ros/ros.h&gt;

class PublishAndSubscribe
{
public:
    PublishAndSubscribe()
    {
        // Define publisher's topic "/published_topic"
        pub_ = n_.advertise&lt;PUBLISHED_MESSAGE_TYPE&gt;("/published_topic", 1);

        // Define subscriber's topic "/subscribed_topic" and declare its callback function
        sub_ = n_.subscribe("/subscribed_topic", 1, &amp;PublishAndSubscribe::callback, this);
    }

    // Define subscriber's callback function
    void callback(const SUBSCRIBED_MESSAGE_TYPE&amp; input)
    {
        PUBLISHED_MESSAGE_TYPE output;
        //.... do something with the input and generate the output...
        pub_.publish(output);
    }

private:
    ros::NodeHandle n_; 
    ros::Publisher pub_;
    ros::Subscriber sub_;

}; // End of class PublishAndSubscribe

int main(int argc, char **argv)
{
    // Initialize ROS node "publish_and_subscribe"
    ros::init(argc, argv, "publish_and_subscribe");

    // Create an instance of PublishAndSubscribe class that will take care of everything
    PublishAndSubscribe SaPObject;

    // Check for an incoming message to the subscriber, or for a service request for the server
    ros::spin();

    return 0;
}
</code></pre></div></div> <h4 id="project2-go-chase-it-">Project2: Go Chase It <a href="https://github.com/SanjarNormuradov/RoboticsSoftwareEngineer_Project2"><i class="fa-brands fa-github"></i></a></h4> <div class="container"> <div class="row"> <div class="align-self-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/project2_go_chase_it-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/project2_go_chase_it-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/project2_go_chase_it-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/project2_go_chase_it.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> </div> <h3 id="4-localization">4. Localization</h3> <p>Types of localization problems, given a known map of the environment:</p> <ul> <li><strong>Local Localization (Position Tracking)</strong>, when the robot has an initial pose and continuously updated this estimate as it moves.</li> <li><strong>Global Localization (Global Pose Estimation)</strong>, when the robot has no initial pose and must determine its location from scratch.</li> <li><strong>Kidnapped Robot Problem</strong>, when the robot has no initial pose and can be moved to unknown location at any time without warning.</li> </ul> <p><strong>Markov Properties</strong> in terms of Localization in Robotics:</p> <ul> <li>static world.</li> <li>noise in sensor readings and motion commands independent of the noise from previous readings and movements.</li> <li>perfect model with no approximation errors, matching the actual dynamics and sensing of the robot.</li> <li>estimation of the current pose depends only on the previous belief and current action, and not on the sequence of events that preceded it.</li> <li>observation (sensor measurement) depends only on the current estimated pose, and not on the sequence of events that preceded it.</li> </ul> <p><strong>Markov Localization</strong>, based on Markov Properties above, maintains a belief distribution (probability distribution) over the set of all possible states (robot’s poses) and updates this belief according to sensor measurements and motion commands. It could be implemented depending on the specific requirements of the environment and the robot’s sensors with:</p> <ul> <li><strong>Kalman Filters</strong> (standard, Extended, Unscented) - a type of Bayesian filter that assumes the robot’s (non-)linear dynamic model and the measurements are subject to Gaussian noise. Computationally efficient.</li> <li><strong>Histogram Filters</strong> - a type of non-parametric Bayesian filter that uses discretized representation of the state space. The belief about the robot’s pose is represented as a probability distribution over this grid. Limited to discretized thus less flexible for continous spaces, and computationally inefficient for high-dimensional spaces.</li> <li><strong>Particle Filters</strong> (Monte-Carlo Localization) - a type of non-parametric Bayesian filter that doesn’t assume any specific distribution (like Gaussian in Kalman filter), thus can represent complex and irregular distributions modelling non-linear and non-Gaussian processes. Set of particles is used to represent the belief distribution. Computationally heavy.</li> </ul> <h4 id="kalman-filter"><strong>Kalman Filter</strong></h4> <p>Given the proper initial estimate and Gaussian noise in measurments and movements<br/> Pros:</p> <ul> <li>computationally efficient - no need for large-scale numerical simulations to make an estimate, as it uses linear equations.</li> <li>sequential processing - data coming continuously in online systems updates sequentially the estimate at each step.</li> <li>sensor fusion - data from multiple sensors (GPS, LIDAR, etc) weighed according to their variance (more accurate measurement is given more weight) and combined together result in Gaussian distribution with a minimized overall variance.</li> </ul> <p>Cons:</p> <ul> <li>non-linear systems needs modifications (EKF, UKF).</li> <li>non-Gaussian noise needs more flexible pose estimate.</li> <li>poor initialization of the base estimate result in inaccurate future estimates.</li> </ul> <p><strong>Types of sensors used in Kalman Filter:</strong></p> <ul> <li>Inertial Measurement Unit (IMU): 3-DoF Gyroscope for angular velocity + 3-DoF Accelerometer for linear acceleration measuring.<br/> \(x = \int\int g \, dt\). The error from double integration might accumulate over time. Check the drift.</li> <li>Inertial and Magnetic Measurement Unit (IMMU): 6-DoF IMU + 3-DoF Magnetometer for magnetic field measuring.</li> <li>Rotary Encoders: measures wheels velocity and position. \(x = \int v \, dt\). Wheel slippage and lockup would lead to inaccurate and noisy measurments.<br/> High-resolution (CPR: counts per revolution) encoders are more sensitive to slippage.</li> <li>Vision Cameras (Stereo, RGB-D), LIDAR: measures distance to obstacles.<br/> Light conditions, surface texture, max-min range, and sensor sensitivity determine the accuracy of measurements.</li> </ul> <h5 id="gaussian-distributions"><strong>Gaussian Distributions:</strong></h5> <ul> <li>univariate with <strong>Mean</strong> \(\mu\) and <strong>Variance</strong> \(\sigma\) <ul> <li> \[p(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)\] </li> </ul> </li> <li>multivariate <ul> <li> \[\text{Mean Vector: } \boldsymbol{\mu} = \begin{bmatrix} \mu_x \\ \mu_y \end{bmatrix}\] </li> <li> \[\text{Covariance Matrix: } \boldsymbol{\Sigma} = \begin{bmatrix} \sigma_x^2 &amp; \rho\sigma_x\sigma_y \\ \rho\sigma_x\sigma_y &amp; \sigma_y^2 \end{bmatrix}\] </li> <li> \[p(\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{2\pi |\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})\right)\] </li> </ul> </li> </ul> <h5 id="univariate-kalman-filter-mean--variance-computation"><strong>Univariate Kalman Filter: Mean &amp; Variance computation:</strong></h5> <p>State Prediction</p> <ul> <li> \[\mu^{\prime} = \mu_1 + \mu_2\] </li> <li> \[\sigma^{\prime \, 2} = \sigma_1^2 + \sigma_2^2\] </li> </ul> <div class="container"> <div class="row"> <div class="col-sm-8 align-self-center offset-sm-2"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/kalman_state_prediction-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/kalman_state_prediction-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/kalman_state_prediction-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/kalman_state_prediction.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>Measurement Update</p> <div class="container"> <div class="row"> <div class="col-sm-8 align-self-center offset-sm-2"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/kalman_measurement_update-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/kalman_measurement_update-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/kalman_measurement_update-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/kalman_measurement_update.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <ul> <li> \[\mu^{\prime} = \frac{r^2 \mu + \sigma^2 v}{r^2 + \sigma^2}\] </li> <li> \[\sigma^{\prime \, 2} = \frac{r^2 \, \sigma^2}{r^2 + \sigma^2}\] </li> </ul> <h5 id="multivariate-kalman-filter-mean--variance-computation"><strong>Multivariate Kalman Filter: Mean &amp; Variance computation:</strong></h5> <p>State Prediction</p> <ul> <li> \[\text{Prior Mean Vector: } \mathbf{x} = \begin{bmatrix} x \\ \dot{x} \\ \ddot{x} \end{bmatrix}\] </li> <li> \[\text{State Prediction (noise-free): } {\begin{bmatrix} x \\ \dot{x} \\ \ddot{x} \end{bmatrix}}^\prime = \begin{bmatrix} 1 &amp; \Delta t &amp; \frac{1}{2} \Delta t^2 \\ 0 &amp; 1 &amp; \Delta t \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} x \\ \dot{x} \\ \ddot{x} \end{bmatrix}\] </li> <li> \[\text{State Prediction Function (noise-free): } \mathbf{F} = \begin{bmatrix} 1 &amp; \Delta t &amp; \frac{1}{2} \Delta t^2 \\ 0 &amp; 1 &amp; \Delta t \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\] </li> <li> \[\text{Covariance Matrix (noise-free): } \mathbf{P}^\prime = \mathbf{F} \mathbf{P} \mathbf{F}^T\] </li> <li> \[\text{State Prediction (with Gaussian noise): } \mathbf{x}^\prime = \mathbf{F} \mathbf{x} + \text{noise}, \ \text{noise} \sim \mathbf{N} (0, \mathbf{Q})\] </li> <li> \[\text{Covariance Matrix (with Gaussian noise): } \mathbf{P}^\prime = \mathbf{F} \mathbf{P} \mathbf{F}^T + \mathbf{Q}\] </li> </ul> <p>Measurement Update</p> <ul> <li> \[\text{Actual Measurement Vector: } \mathbf{z}\] </li> <li> \[\begin{gather*} \text{Expected Measurement Vector: } \mathbf{H} \mathbf{x}^\prime, \\ \text{where} \ \mathbf{H} \text{ is Measurement Function, mapping the state to an observation} \end{gather*}\] </li> <li> \[\text{Measurement Residual Vector: } \mathbf{y} = \mathbf{z} - \mathbf{H} \mathbf{x}^\prime\] </li> <li> \[\text{Covariance Matrix (with Gaussian noise): } \mathbf{S}^\prime = \mathbf{H} \mathbf{P}^\prime \mathbf{H}^T + \mathbf{R}\] </li> </ul> <p>Kalman Gain Calculation</p> <ul> <li> \[\text{Kalman Gain: } \mathbf{K} = \mathbf{P}^\prime \mathbf{H}^T \mathbf{S}^{-1}\] </li> </ul> <p>Posterior Mean and Covariance Calculations</p> <ul> <li> \[\text{Posterior Mean Vector: } \hat{\mathbf{x}} = \mathbf{x}^\prime + \mathbf{K} \mathbf{y}\] </li> <li> \[\text{Posterior Covariance Matrix: } \hat{\mathbf{P}} = (\mathbf{I} - \mathbf{K} \mathbf{H}) \mathbf{P}^\prime\] </li> </ul> <h6 id="analysis-of-kalman-gain"><strong>Analysis of Kalman Gain:</strong></h6> <ol> <li>Perfect sensor measurements \(\mathbf{R} = \begin{bmatrix} 0 \end{bmatrix}\) <ul> <li> \[\mathbf{S}^\prime = \mathbf{H} \mathbf{P}^\prime \mathbf{H}^T + \mathbf{R} = \mathbf{H} \mathbf{P}^\prime \mathbf{H}^T\] </li> <li> \[\mathbf{K} = \mathbf{P}^\prime \mathbf{H}^T \mathbf{S}^{-1} = \mathbf{P}^\prime \mathbf{H}^T (\mathbf{H} \mathbf{P}^\prime \mathbf{H}^T)^{-1} = \mathbf{P}^\prime \mathbf{H}^T (\mathbf{H}^{T - 1} \mathbf{P}^{\prime - 1} \mathbf{H}^{-1}) = \mathbf{H}^{-1}\] </li> <li> \[\begin{gather*} \hat{\mathbf{x}} = \mathbf{x}^\prime + \mathbf{K} \mathbf{y} = \mathbf{x}^\prime + \mathbf{H}^{-1} (\mathbf{z} - \mathbf{H} \mathbf{x}^\prime) = \mathbf{H}^{-1} \mathbf{z}, \\ \text{rely entirely on sensor measurements, state prediction is unreliable} \end{gather*}\] </li> </ul> </li> <li>Noisy sensor measurements \(\mathbf{R} = \begin{bmatrix} \infty \end{bmatrix}\) <ul> <li> \[\mathbf{S}^\prime = \mathbf{H} \mathbf{P}^\prime \mathbf{H}^T + \mathbf{R} = \begin{bmatrix} \infty \end{bmatrix}\] </li> <li> \[\mathbf{K} = \mathbf{P}^\prime \mathbf{H}^T \mathbf{S}^{-1} = \begin{bmatrix} 0 \end{bmatrix}\] </li> <li> \[\begin{gather*} \hat{\mathbf{x}} = \mathbf{x}^\prime + \mathbf{K} \mathbf{y} = \mathbf{x}^\prime, \\ \text{rely entirely on state prediction, sensor measurement is unreliable} \end{gather*}\] </li> </ul> </li> </ol> <h4 id="extended-kalman-filter"><strong>Extended Kalman Filter</strong></h4> <p>Non-linear motion and measurment functions can be used to update the mean vector, but need to be linearized over a small section to update the covariance matrix. Otherwise, Gaussian distribution would turn into non-Gaussian distribution, dealing with which is computationally inefficient.\</p> <ul> <li> \[\begin{gather*} \text{Linearization is achieved through Taylor series:} \\ \mathbf{T}(\mathbf{x}) = f(\boldsymbol{\mu}) + (\mathbf{x} - \boldsymbol{\mu})^T \, \nabla f(\boldsymbol{\mu}) + \frac{1}{2!} (\mathbf{x} - \boldsymbol{\mu})^T \, \nabla^2 f(\boldsymbol{\mu}) (\mathbf{x} - \boldsymbol{\mu}) + ... \end{gather*}\] </li> <li> \[\begin{gather*} \text{where } \nabla f(\mathbf{\boldsymbol{\mu}}) \text{ is the gradient of } f \text{ evaluated at } \boldsymbol{\mu}, \text{which is a Jacobian matrix } \mathbf{J} \text{ of partial derivatives:} \\ \nabla f(\mathbf{\boldsymbol{\mu}}) = \mathbf{J} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n} \\ \frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_2}{\partial x_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f_m}{\partial x_1} &amp; \frac{\partial f_m}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n} \end{bmatrix} \end{gather*}\] </li> </ul> <p><strong>Example:</strong></p> <div class="container"> <div class="row"> <div class="col-sm-8 align-self-center offset-sm-2"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/ekf_example_drone-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/ekf_example_drone-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/ekf_example_drone-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/ekf_example_drone.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>Measurement Update</p> <ul> <li> \[\text{State Vector: } \mathbf{x} = \begin{bmatrix} \phi \\ \dot{y} \\ y \end{bmatrix}, \text{where } \phi \text{ is the roll angle}\] </li> <li> \[\text{Measurement Function: } h(\mathbf{x}) = \begin{bmatrix} \frac{wall - y}{ \cos{\phi}} \end{bmatrix}\] </li> <li> \[\text{Measurement Function Linearization: } h(\mathbf{x}) \simeq h(\mathbf{x}) + (\mathbf{x} - \boldsymbol{\mu})^T \, \nabla h(\mathbf{\boldsymbol{\mu}})\] </li> <li> \[\nabla h(\mathbf{\boldsymbol{\mu}}) = \mathbf{H} = \begin{bmatrix} \frac{\partial h}{\partial \phi} &amp; \frac{\partial h}{\partial \dot{y}} &amp; \frac{\partial h}{\partial y} \end{bmatrix} = \begin{bmatrix} \frac{\sin{\phi}}{\cos^2{\phi}} (wall - y) &amp; 0 &amp; \frac{-1}{\cos{\phi}} \end{bmatrix}\] </li> </ul> <h5 id="extended-kalman-filter-equations"><strong>Extended Kalman Filter Equations:</strong></h5> <p>State Prediction</p> <ul> <li> \[\mathbf{x}^\prime = f(\mathbf{x})\] </li> <li> \[\mathbf{P}^\prime = \mathbf{F} \mathbf{P} \mathbf{F}^T + \mathbf{Q}\] </li> </ul> <p>Measurement Update</p> <ul> <li> \[\mathbf{y} = \mathbf{z} - h(\mathbf{x}^\prime)\] </li> <li> \[\mathbf{S}^\prime = \mathbf{H} \mathbf{P}^\prime \mathbf{H}^T + \mathbf{R}\] </li> </ul> <p>Kalman Gain Calculation</p> <ul> <li> \[\mathbf{K} = \mathbf{P}^\prime \mathbf{H}^T \mathbf{S}^{-1}\] </li> </ul> <p>Posterior Mean and Covariance Calculations</p> <ul> <li> \[\hat{\mathbf{x}} = \mathbf{x}^\prime + \mathbf{K} \mathbf{y}\] </li> <li> \[\hat{\mathbf{P}} = (\mathbf{I} - \mathbf{K} \mathbf{H}) \mathbf{P}^\prime\] </li> </ul> <h5 id="lab-extended-kalman-filter"><strong>Lab: Extended Kalman Filter:</strong></h5> <p>Used ROS packages:</p> <ul> <li><a href="http://wiki.ros.org/turtlebot_teleop">turtlebot_gazebo</a> to spawn TurtleBot 2 in a Gazebo environment.</li> <li><a href="https://github.com/turtlebot/turtlebot">turtlebot_teleop</a> to publish robot control commands to <code class="language-plaintext highlighter-rouge">/cmd_vel_mux/input/teleop</code> ROS topic from keyboard.</li> <li><strong>rviz</strong> to visualize the estimated robot poses.</li> <li><a href="http://wiki.ros.org/robot_pose_ekf">robot_pose_ekf</a> to estimate the robot poses from teleop motion commands and fusion of two sensor measurements: IMU (<code class="language-plaintext highlighter-rouge">/mobile_base/sensors/imu_data</code>) and rotary encoders (<code class="language-plaintext highlighter-rouge">/odom</code>).</li> <li><strong>odom_to_trajectory</strong> (by Udacity) to generate trajectory paths from filtered (<code class="language-plaintext highlighter-rouge">/ekfpath</code>) and unfiltered (<code class="language-plaintext highlighter-rouge">/odompath</code>) poses.</li> </ul> <div class="container"> <div class="row"> <div class="align-self-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/ekf_lab_overall_graph-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/ekf_lab_overall_graph-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/ekf_lab_overall_graph-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/ekf_lab_overall_graph.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="align-self-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/ekf_lab_sim-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/ekf_lab_sim-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/ekf_lab_sim-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/ekf_lab_sim.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p><br/></p> <h4 id="monte-carlo-localization"><strong>Monte-Carlo Localization</strong></h4> <p><strong>Pros:</strong></p> <ul> <li>can be used for both local and global localization problems, while EKF is for local only.</li> <li>not limited to Gaussian noise as EKF.</li> <li>memory &amp; resolution can be control through the number of particles.</li> <li>particles represent belief distribution of where the robot might be.</li> <li>easier to implement than EKF.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li>computationally and memory-wise expensive (especially to achieve high resolution).</li> <li>poor resampling result in particle deprevation and worse resolution.</li> </ul> <p>For more information, check out the paper <a href="http://robots.stanford.edu/papers/thrun.robust-mcl.pdf">“Robust Monte-Carlo Localization for Mobile Robots”</a></p> <div class="container"> <div class="row"> <div class="col-sm-8 align-self-center offset-sm-2"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_2Dmap_with_particles-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_2Dmap_with_particles-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_2Dmap_with_particles-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/mcl_2Dmap_with_particles.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm-8 align-self-center offset-sm-2"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_2Dmap_with_converged_particles-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_2Dmap_with_converged_particles-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_2Dmap_with_converged_particles-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/mcl_2Dmap_with_converged_particles.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> Paricles with higher weight, which is defined by how close the predicted pose to the robot actual pose, are more likely to survive during the resampling process. </div> <h6 id="types-of-range-sensor-lidar-noise"><strong>Types of Range Sensor (LIDAR) Noise</strong></h6> <ol type="a"> <li>Local Measurement Noise: caused by limited resolution of range sensors; atmespheric effects on the measurement signals.</li> <li>Unexpected Objects: caused by dynamic environment (e.g. people) in static map.</li> <li>Sensor Failures: caused by black, light-absorbing objects; bright sunlight; smooth surfaces such as walls, which effectively becomes a mirror (surface material, sensitivity of the sensor).</li> <li>Random Measurements: caused by cross-talk between different sensors; phantom readings from signals bounced off walls.</li> </ol> <div class="container"> <div class="row"> <div class="col-sm-8 align-self-center offset-sm-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_sensor_noise_types-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_sensor_noise_types-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_sensor_noise_types-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/mcl_sensor_noise_types.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> $$ z^k_t \text{ - reading from } k^{th} \text{ beam sensor at time } t, \, x_t \text{ - robot position at time } t, m \text{ - map} $$ </div> <div class="container"> <div class="row"> <div class="col-sm-8 align-self-center offset-sm-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_overall_sensor_model-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_overall_sensor_model-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_overall_sensor_model-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/mcl_overall_sensor_model.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm-8 align-self-center offset-sm-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_sensor_model_function-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_sensor_model_function-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_sensor_model_function-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/mcl_sensor_model_function.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> Sensor Model that takes into account different types of noises. </div> <h5 id="mcl-pseudo-code"><strong>MCL Pseudo-Code</strong></h5> <div class="container"> <div class="row"> <div class="col-sm-8 align-self-center offset-sm-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_pseudo_code-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_pseudo_code-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_pseudo_code-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/mcl_pseudo_code.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> State Prediction (4) through Motion Model; Measurement Update (5) through Sensor Model; Resampling (8-11). </div> <div class="container"> <div class="row"> <div class="col-sm-8 align-self-center offset-sm-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_beam_sensor_model-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_beam_sensor_model-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_beam_sensor_model-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/mcl_beam_sensor_model.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> LIDAR Sensor Model. </div> <div class="container"> <div class="row"> <div class="col-sm-8 align-self-center offset-sm-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_low_variance_resampler-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_low_variance_resampler-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/mcl_low_variance_resampler-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/mcl_low_variance_resampler.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> Low-Variance Resampler. </div> <h4 id="project3-where-am-i-">Project3: Where Am I <a href="https://github.com/SanjarNormuradov/RoboticsSoftwareEngineer_Project3"><i class="fa-brands fa-github"></i></a></h4> <div class="container"> <div class="row"> <div class="align-self-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/course/udacity_robotics_swe/project3_where_am_i-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/course/udacity_robotics_swe/project3_where_am_i-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/course/udacity_robotics_swe/project3_where_am_i-1400.webp"/> <img src="/assets/img/projects/course/udacity_robotics_swe/project3_where_am_i.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> </div> </article></div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> &copy; Copyright 2026 Sanjar Normuradov . Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>